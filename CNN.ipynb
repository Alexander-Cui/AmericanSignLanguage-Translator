{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import split_folders\n",
    "import sklearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the paths for images\n",
    "\n",
    "path_ASL_alphabet_train = \"D:\\Github\\MAIS202\\AmericanSignLanguage-Translator\\Images\\asl-alphabet\\asl_alphabet_train\\asl_alphabet_train\"\n",
    "path_ASL_alphabet_test = \"D:\\Github\\MAIS202\\AmericanSignLanguage-Translator\\Images\\asl-alphabet\\asl_alphabet_test\\asl_alphabet_test\"\n",
    "\n",
    "#Note: TODO Dataset for digits is saved as a .npy file. Will be done for next deliverable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Loading images from a folder\n",
    "# def load_images (folder):\n",
    "#     images = []\n",
    "#     for image_name in os.listdir(folder):\n",
    "#         img = cv2.imread(os.path.join(folder,image_name))\n",
    "#         if img is not None:\n",
    "#             images.append(img)\n",
    "    \n",
    "#     return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir ='D:\\\\Github\\\\MAIS202\\\\AmericanSignLanguage-Translator\\\\Images\\\\asl-alphabet\\\\asl_alphabet_train\\\\train'\n",
    "sample_path = 'D:\\\\Github\\\\MAIS202\\\\AmericanSignLanguage-Translator\\\\Images\\\\asl-alphabet\\\\asl_alphabet_train\\\\sample_split'\n",
    "\n",
    "\n",
    "for folder_name in os.listdir(sample_path):\n",
    "    mk_dir_path = os.path.join(sample_path,folder_name)\n",
    "       \n",
    "    shutil.rmtree(mk_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'D:\\\\Github\\\\MAIS202\\\\AmericanSignLanguage-Translator\\\\Images\\\\asl-alphabet\\\\asl_alphabet_train\\\\sample\\\\A'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a8f8a8734250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmk_dir_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'D:\\\\Github\\\\MAIS202\\\\AmericanSignLanguage-Translator\\\\Images\\\\asl-alphabet\\\\asl_alphabet_train\\\\sample\\\\A'"
     ]
    }
   ],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dir ='D:\\\\Github\\\\MAIS202\\\\AmericanSignLanguage-Translator\\\\Images\\\\asl-alphabet\\\\asl_alphabet_train\\\\train'\n",
    "sample_path = 'D:\\\\Github\\\\MAIS202\\\\AmericanSignLanguage-Translator\\\\Images\\\\asl-alphabet\\\\asl_alphabet_train\\\\sample'\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "for folder_name in os.listdir(train_dir):\n",
    "    mk_dir_path = os.path.join(sample_path,folder_name)\n",
    "    \n",
    "    \n",
    "    os.mkdir(mk_dir_path)\n",
    "    \n",
    "    \n",
    "    for image_name in os.listdir(train_dir+'\\\\'+ folder_name)[::10]:\n",
    "        \n",
    "        \n",
    "        img = cv2.imread(os.path.join(train_dir+'\\\\'+ folder_name,image_name) )\n",
    "        \n",
    "        \n",
    "        cv2.imwrite(os.path.join(mk_dir_path,image_name)+'.jpg',img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\\\Github\\\\MAIS202\\\\AmericanSignLanguage-Translator\\\\Images\\\\asl-alphabet\\\\asl_alphabet_train\\\\sample_split'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 29\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 20\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=29, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),    \n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 3.1868 Acc: 0.1248\n",
      "val Loss: 2.7891 Acc: 0.2802\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 2.6369 Acc: 0.3297\n",
      "val Loss: 2.2503 Acc: 0.4432\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 2.3239 Acc: 0.4330\n",
      "val Loss: 1.9113 Acc: 0.5366\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 2.1137 Acc: 0.4854\n",
      "val Loss: 1.7860 Acc: 0.5237\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.9550 Acc: 0.5165\n",
      "val Loss: 1.5774 Acc: 0.5776\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.8637 Acc: 0.5362\n",
      "val Loss: 1.4393 Acc: 0.6343\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.7716 Acc: 0.5639\n",
      "val Loss: 1.3659 Acc: 0.6473\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 1.6930 Acc: 0.5843\n",
      "val Loss: 1.2835 Acc: 0.6681\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 1.6395 Acc: 0.5845\n",
      "val Loss: 1.2109 Acc: 0.6889\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 1.5922 Acc: 0.5852\n",
      "val Loss: 1.1829 Acc: 0.6990\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 1.5499 Acc: 0.6037\n",
      "val Loss: 1.1549 Acc: 0.6868\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 1.5333 Acc: 0.6025\n",
      "val Loss: 1.0889 Acc: 0.7119\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 1.4851 Acc: 0.6214\n",
      "val Loss: 1.1051 Acc: 0.7033\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 1.4366 Acc: 0.6391\n",
      "val Loss: 1.0368 Acc: 0.7256\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 1.4188 Acc: 0.6360\n",
      "val Loss: 1.0263 Acc: 0.7155\n",
      "\n",
      "Training complete in 5m 29s\n",
      "Best val Acc: 0.725575\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloaders):\n",
    "\n",
    "    ids = np.array([])\n",
    "    pred = np.array([])\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['test']:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "        \n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Iterate over data.\n",
    "        for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # print(i)\n",
    "            # print(dataloaders[phase].dataset.samples[i])\n",
    "            # print(dataloaders[phase].dataset.samples[i][0].split(\"/\")[3].split(\".\")[0])\n",
    "\n",
    "            # zero the parameter gradients\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                ids = np.concatenate( (ids, labels.cpu().numpy() ), axis=None)\n",
    "                \n",
    "                pred = np.concatenate((pred, preds.cpu().numpy()), axis=None)\n",
    "                \n",
    "                # print(preds.cpu().numpy())\n",
    "                # print(labels.cpu().data.numpy())\n",
    "            # statistics\n",
    "            \n",
    "\n",
    "    return model, ids, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "image_datasets_test = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['val', 'test']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict_test = {x: torch.utils.data.DataLoader(image_datasets_test[x], batch_size=1, shuffle=False, num_workers=4) for x in ['val', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ids, labels = eval_model(model_ft, dataloaders_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  3.  3.  3.\n",
      "  3.  3.  4.  4.  4.  4.  5.  5.  5.  5.  5.  6.  6.  6.  6.  6.  7.  7.\n",
      "  7.  7.  7.  8.  8.  8.  8.  8.  9.  9.  9.  9. 10. 10. 10. 10. 10. 11.\n",
      " 11. 11. 11. 11. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 14. 14. 14. 14.\n",
      " 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 17. 17. 17. 17. 17. 18. 18. 18.\n",
      " 18. 18. 19. 19. 19. 19. 20. 20. 20. 20. 20. 21. 21. 21. 21. 21. 22. 22.\n",
      " 22. 22. 22. 23. 23. 23. 23. 23. 24. 24. 24. 24. 25. 25. 25. 25. 25. 26.\n",
      " 26. 26. 26. 26. 27. 27. 27. 27. 27. 28. 28. 28. 28. 28.]\n",
      "[ 0.  0.  4.  0.  0.  1.  4.  1.  1.  1.  2. 26.  2.  2.  2.  3.  3.  3.\n",
      "  3.  3.  4. 25. 12.  4.  5.  5.  5. 10.  5.  7.  6.  7.  6.  6.  7.  7.\n",
      "  7.  7.  7.  8. 11.  9.  8. 10. 25. 25.  9.  7. 10. 10. 17. 10. 10.  9.\n",
      " 19. 11. 11. 11. 13. 12. 12. 13. 12. 13.  7. 13. 13. 13. 14.  7. 14. 14.\n",
      " 15. 15. 16. 15. 15. 14. 16. 16. 16. 16. 17. 17. 17. 17. 17. 18.  4. 18.\n",
      " 18. 18. 19.  4. 19.  6. 20. 22. 20. 17. 17.  6. 22. 21. 10. 17. 26. 17.\n",
      " 22. 22. 22. 23. 20. 23. 10.  9. 24.  4. 11. 24. 25.  4.  7. 25. 25. 16.\n",
      " 26. 26. 26. 26. 27. 27. 27. 27. 27. 24. 28. 28. 28. 25.]\n"
     ]
    }
   ],
   "source": [
    "print(ids[::10])\n",
    "print(labels[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  0  0  0  6  1  1  1  0  1  1  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 1 33  0  0  5  0  0  0  0  2  1  0  0  0  0  0  0  1  0  0  5  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0 40  0  0  2  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "   0  1  3  0  0]\n",
      " [ 0  0  0 38  0  3  0  0  2  2  0  0  0  0  0  0  0  2  0  0  0  0  0  1\n",
      "   0  0  0  0  0]\n",
      " [ 2  0  0  0 39  1  1  1  0  0  0  0  1  0  0  0  0  0  1  1  0  0  0  0\n",
      "   0  1  0  0  0]\n",
      " [ 0  0  0  0  0 40  0  1  0  1  2  1  0  0  0  0  0  0  0  1  0  2  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 34 10  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  2 46  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 1  0  0  0  4  0  1  0 23  4  6  2  0  0  0  0  0  5  0  0  0  0  0  1\n",
      "   0  1  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  2  1 38  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "   1  2  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 44  0  0  0  0  0  0  4  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  3  1  0  0  1  0 34  0  0  0  0  0  1  0  1  4  0  0  1\n",
      "   2  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  1  0  2  0  0 24 15  1  0  0  1  1  0  0  0  0  0\n",
      "   0  2  0  0  0]\n",
      " [ 0  0  0  0  2  0  0  1  0  0  0  0  5 37  1  0  0  1  1  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  1  2  0  1  0  2  0  2  0  0  0  0 37  0  0  0  0  0  1  0  1  0\n",
      "   0  0  1  0  0]\n",
      " [ 0  0  1  2  1  0  2  1  0  1  0  0  0  0  4 32  4  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0 47  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0 43  0  0  2  0  0  0\n",
      "   0  1  0  0  0]\n",
      " [ 3  0  0  0  6  0  0  1  0  7  0  0  1  0  0  0  0  0 25  1  0  0  0  1\n",
      "   0  1  1  0  1]\n",
      " [ 0  0  0  0  5  0  4  0  0  2  0  4  0  0  0  0  0  0  1 27  0  0  0  2\n",
      "   2  1  0  0  0]\n",
      " [ 0  1  0  0  1  1  1  1  0  0  1  0  0  0  0  0  0 17  0  0 22  1  2  0\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  2  0  1  2 15  4  0  0  0  0  0  3  0  0  2 13  3  2\n",
      "   0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  4  0  0  0  0  0  0  5  0  0  1  1 35  0\n",
      "   0  0  1  0  0]\n",
      " [ 3  0  0  0  2  0  2  1  0  4  1  0  0  0  0  0  0  6  1  3  4  1  0 18\n",
      "   0  1  0  1  0]\n",
      " [ 0  0  0  0  1  0  5  0  0  1  1  5  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "  33  1  0  0  0]\n",
      " [ 0  0  0  0  5  0  2  6  0  6  0  0  0  0  1  0  0  3  2  0  1  0  0  0\n",
      "   2 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  2  1  0  0  0  0  0  1\n",
      "   0  0 43  0  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 46  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  1  0  0  0  0  1  0  0  2  0  2  0  0  0  0\n",
      "   1  2  1  0 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7826    0.7500    0.7660        48\n",
      "         1.0     0.9706    0.6875    0.8049        48\n",
      "         2.0     0.9524    0.8333    0.8889        48\n",
      "         3.0     0.9048    0.7917    0.8444        48\n",
      "         4.0     0.5000    0.8125    0.6190        48\n",
      "         5.0     0.7273    0.8333    0.7767        48\n",
      "         6.0     0.5574    0.7083    0.6239        48\n",
      "         7.0     0.5974    0.9583    0.7360        48\n",
      "         8.0     0.8519    0.4792    0.6133        48\n",
      "         9.0     0.4578    0.7917    0.5802        48\n",
      "        10.0     0.5714    0.9167    0.7040        48\n",
      "        11.0     0.6800    0.7083    0.6939        48\n",
      "        12.0     0.7742    0.5000    0.6076        48\n",
      "        13.0     0.6981    0.7708    0.7327        48\n",
      "        14.0     0.8043    0.7708    0.7872        48\n",
      "        15.0     1.0000    0.6667    0.8000        48\n",
      "        16.0     0.8868    0.9792    0.9307        48\n",
      "        17.0     0.4526    0.8958    0.6014        48\n",
      "        18.0     0.7353    0.5208    0.6098        48\n",
      "        19.0     0.7500    0.5625    0.6429        48\n",
      "        20.0     0.5116    0.4583    0.4835        48\n",
      "        21.0     0.7222    0.2708    0.3939        48\n",
      "        22.0     0.8537    0.7292    0.7865        48\n",
      "        23.0     0.6667    0.3750    0.4800        48\n",
      "        24.0     0.8049    0.6875    0.7416        48\n",
      "        25.0     0.5882    0.4167    0.4878        48\n",
      "        26.0     0.8600    0.8958    0.8776        48\n",
      "        27.0     0.9583    0.9583    0.9583        48\n",
      "        28.0     0.9737    0.7708    0.8605        48\n",
      "\n",
      "    accuracy                         0.7069      1392\n",
      "   macro avg     0.7446    0.7069    0.7046      1392\n",
      "weighted avg     0.7446    0.7069    0.7046      1392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print (metrics.confusion_matrix(ids,labels) )\n",
    "\n",
    "print (metrics.classification_report(ids,labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
